{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "298aa572-0f53-4e99-8d5f-0f8b9f0a52ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you have to make the module discoverable to load the classes below:\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "os.chdir('../../../nucleotran/')\n",
    "sys.path.append('./src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ecdcfa7-2a38-43a7-abad-dee6cd736fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataloaders import CoverageDatasetHDF5\n",
    "\n",
    "from features.nucleotide import DNATokenizer\n",
    "from torch import tensor\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import Embedding\n",
    "from torch import nn\n",
    "from torch import permute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e04e4e47-a0c5-40b8-a43c-5b2bb1484dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6d0a9d9-fea3-4cef-8aae-ef8e54e3db81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import RichProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54bc30e7-f25b-4526-a412-405eb7265c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the \"sequence order\" or \"token/word length\"\n",
    "seq_order = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a968967-4d6b-4816-a753-fd27d7088585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the tokenizer defines the token length, and the stride\n",
    "dnatokenizer = DNATokenizer(seq_order=seq_order, stride=1, allow_N=False)\n",
    "\n",
    "# dna_embed will help us convert the token-representation to one-hot representation\n",
    "W, mapping = dnatokenizer.get_one_hot_weights_matrix(N_max=0)\n",
    "dna_embed = Embedding.from_pretrained(tensor(W),freeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2875e9b-4656-41db-8c1a-3d3e261ffc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BED-file contains 10000 regions.\n",
      "93.250% of regions have at least 1 label.\n"
     ]
    }
   ],
   "source": [
    "dataset = CoverageDatasetHDF5('data/processed/GRCh38/toydata/regions.bed',\n",
    "                              '/dhc/dsets/reference_genomes/ensembl/Homo_sapiens.GRCh38.dna.primary_assembly_renamed.fa',\n",
    "                              'data/processed/GRCh38/toydata/overlaps.h5',\n",
    "                              dna_tokenizer=dnatokenizer,\n",
    "                              random_shift=0,\n",
    "                              random_reverse_complement=True,\n",
    "                              transform=lambda x: permute(dna_embed(tensor(x)), [0,2,1]), # the loader serves up the data in \"channels-last\" format, but pytorch wants \"channels-first\", so we permute\n",
    "                              target_transform=tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f7ec03f-b089-436c-8a93-939f4d4f7e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.bedseqloader.resize(128+(seq_order-1)) # add one to keep the input sequence length the same when using bi-nucleotide encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23a21871-a93a-41b2-a5c8-959ba8f531e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we decide we want ot use chromosomes 9 and 10 as the test set\n",
    "i_train, i_test = dataset.train_test_split_chromosomes(['9','10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eda48e38-a351-41dd-9c80-5aba417a503b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    1,    2, ..., 9997, 9998, 9999])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b4b2480-cdb2-493e-b234-0022094ded7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train len: 8896\n",
      "val len: 200\n",
      "test len: 904\n"
     ]
    }
   ],
   "source": [
    "i_val = i_train[(np.arange(len(i_train)) % 1000) < 20] # sample consecutive regions for the validation set, which is a sub-set of the training set\n",
    "i_train = np.setdiff1d(i_train, i_val)\n",
    "\n",
    "print(f'train len: {len(i_train)}')\n",
    "print(f'val len: {len(i_val)}')\n",
    "print(f'test len: {len(i_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bf93850-f3b2-4861-b228-02597484d0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2048 # we could probably fit much more, but this is just an example...\n",
    "\n",
    "\n",
    "train_sampler = torch.utils.data.sampler.BatchSampler(\n",
    "                torch.utils.data.sampler.SubsetRandomSampler(i_train),\n",
    "                batch_size=batch_size,\n",
    "                drop_last=False)\n",
    "\n",
    "val_sampler = torch.utils.data.sampler.BatchSampler(\n",
    "                torch.utils.data.sampler.SubsetRandomSampler(i_val),\n",
    "                batch_size=batch_size,\n",
    "                drop_last=False)\n",
    "\n",
    "test_sampler = torch.utils.data.sampler.BatchSampler(\n",
    "                torch.utils.data.sampler.SubsetRandomSampler(i_test),\n",
    "                batch_size=batch_size,\n",
    "                drop_last=False) \n",
    "\n",
    "train_dataloader = DataLoader(dataset, sampler=train_sampler)\n",
    "val_dataloader = DataLoader(dataset, sampler=val_sampler)\n",
    "test_dataloader = DataLoader(dataset, sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef8572f6-bb2a-48a4-9460-b29ed213da7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_init_bias(i_train, dataset, n_sample=1000):\n",
    "    \n",
    "    \"\"\"\n",
    "    Estimates the class-frequencies from n_sample samples, and returns the corresponding bias vector to initialize the model with\n",
    "    \"\"\"\n",
    "    \n",
    "    _, y = dataset[np.random.choice(i_train, size=min(n_sample, len(i_train)), replace=False)]\n",
    "    frq = (y.detach().numpy().sum(axis=0) + 1) / len(y)\n",
    "    \n",
    "    b = -np.log((1/frq - 1))\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c5adbbc-454d-4590-8378-5bc7bb3c23ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = get_init_bias(i_train, dataset, n_sample=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "003d4e83-4565-47e4-8fe3-24ae2e353916",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DinucleotideBaseline(pl.LightningModule):\n",
    "    \n",
    "    \"\"\"\n",
    "    baseline model that works only with dinulceotide frequencies.\n",
    "    \n",
    "    expects input to be shape (16, l), where l is the sequence length\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_length=128, n_classes=1, init_bias=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.avgpool = nn.AvgPool1d(kernel_size=input_length, stride=1, padding=0)\n",
    "        self.linear = nn.Linear(in_features=16, out_features=n_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        if init_bias is not None:\n",
    "            self.linear.bias.data = init_bias\n",
    "            \n",
    "        self.loss_fun = nn.BCELoss(reduction='mean')\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(self.avgpool(x),1,2)\n",
    "        x = self.linear(x)\n",
    "        out = self.sigmoid(x)\n",
    "        return out\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x[0] # TODO: why does the loaded data have an additional dimension in front (?)\n",
    "        y = y[0]\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fun(y_hat, y)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x[0] # TODO: why does the loaded data have an additional dimension in front (?)\n",
    "        y = y[0]\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fun(y_hat, y)\n",
    "        self.log(\"val_loss\", loss, on_step=False, prog_bar=True, on_epoch=True, logger=True)\n",
    "    \n",
    "    def configure_optimizers(self, lr=None):\n",
    "        if lr is None:\n",
    "            lr = 1e-2\n",
    "        return torch.optim.Adam(self.parameters(), lr=lr)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f41a8a65-51cd-4012-9aa3-1bde1a6ae1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=20, log_every_n_steps=1, val_check_interval=5, callbacks=[RichProgressBar()])\n",
    "model = DinucleotideBaseline(input_length=128, n_classes=dataset.labelloader.n_labels, init_bias=tensor(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57df685f-4046-477d-8c14-e4def3ea4b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DinucleotideBaseline(\n",
       "  (avgpool): AvgPool1d(kernel_size=(128,), stride=(1,), padding=(0,))\n",
       "  (linear): Linear(in_features=16, out_features=2106, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (loss_fun): BCELoss()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8120af41-8007-41c3-abf3-1fc35e6fb9b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Epoch 19  <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">6/6</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:00:02 • 0:00:00</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">3.29it/s</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">loss: 0.0933 v_num: 453849           </span>\n",
       "                                                                              <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">train_loss_step: 0.092 val_loss:     </span>\n",
       "                                                                              <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0.083 train_loss_epoch: 0.093        </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Epoch 19  \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m6/6\u001b[0m \u001b[38;5;245m0:00:02 • 0:00:00\u001b[0m \u001b[38;5;249m3.29it/s\u001b[0m \u001b[37mloss: 0.0933 v_num: 453849           \u001b[0m\n",
       "                                                                              \u001b[37mtrain_loss_step: 0.092 val_loss:     \u001b[0m\n",
       "                                                                              \u001b[37m0.083 train_loss_epoch: 0.093        \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "942787b0-8a62-4552-baa0-6e3f41e2c8d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AA   -0.589999\n",
       "AC   -0.122978\n",
       "AG    0.050020\n",
       "AT   -0.675674\n",
       "CA   -0.197982\n",
       "CC    0.540073\n",
       "CG    0.669514\n",
       "CT    0.086498\n",
       "GA    0.067287\n",
       "GC    0.566212\n",
       "GG    0.566071\n",
       "GT   -0.126278\n",
       "TA   -0.672120\n",
       "TC    0.007918\n",
       "TG   -0.136977\n",
       "TT   -0.554329\n",
       "dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(model.linear.weight.detach().numpy().mean(axis=0), index=list(dnatokenizer.mapping.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6591a99e-f2ef-4283-afd2-b97c5d9f4485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add evaluation step to the model\n",
    "# TODO: load the metadata, visualize evaluation metrics stratified by different groups"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
